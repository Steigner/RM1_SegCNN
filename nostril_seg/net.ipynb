{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "\n",
    "import random\n",
    "import cv2\n",
    "import wandb\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import segmentation_models as sm\n",
    "import tensorflow_advanced_segmentation_models as tasm\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"[INFO] CNN Seg. Model Script\")\n",
    "print(\"[INFO] Author: Martin Juricek\")\n",
    "print(\"[INFO] Supervisor: Ing. Roman Parak\")\n",
    "print(\"[INFO] IACS FME BUT @2022\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "class Image_Load(object):\n",
    "    def get_data(self, image_dir, mask_dir):\n",
    "        img_paths = sorted(\n",
    "            [\n",
    "                os.path.join(image_dir, fname)\n",
    "                for fname in os.listdir(image_dir)\n",
    "                if fname.endswith(\".jpg\")\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        mask_paths = sorted(\n",
    "            [\n",
    "                os.path.join(mask_dir, fname)\n",
    "                for fname in os.listdir(mask_dir)\n",
    "                if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(\"[INFO] Number of loaded samples:\", len(img_paths))\n",
    "\n",
    "        return img_paths, mask_paths\n",
    "\n",
    "class Image_Augment(object):\n",
    "    def __aug_settings(self, train):\n",
    "        if train == True:\n",
    "            transform = A.Compose([\n",
    "                #A.Transpose(p=1),\n",
    "                A.CLAHE(p=0.8),\n",
    "                A.GridDistortion(p=1),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.OneOf(\n",
    "                    [\n",
    "                        A.CLAHE(p=1),\n",
    "                        A.RandomBrightness(p=1),\n",
    "                        A.RandomGamma(p=1),\n",
    "                    ],\n",
    "                    p=0.9,\n",
    "                ),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "            ])\n",
    "        \n",
    "        else:\n",
    "            transform = A.Compose([\n",
    "                A.PadIfNeeded(480, 640),\n",
    "                A.Resize(480, 640, always_apply=True)\n",
    "            ])\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def aug_process(self, img_paths, mask_paths, num, train):\n",
    "        transform = self.__aug_settings(train)\n",
    "        \n",
    "        target = zip(img_paths, mask_paths)\n",
    "\n",
    "        j = 0\n",
    "        images, masks = [], []\n",
    "\n",
    "        for image_path, mask_path in target:\n",
    "            j+=1 \n",
    "            img = cv2.imread(image_path)\n",
    "            mask = cv2.imread(mask_path)\n",
    "            \n",
    "            for i in range(num):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "\n",
    "                images.append(transformed_image)\n",
    "                masks.append(transformed_mask[:, :, 0])\n",
    "                \n",
    "        print(\"[INFO] Were generated images: \" + str(len(images)))\n",
    "        print(\"[INFO] Were generated masks: \" + str(len(masks)))\n",
    "\n",
    "        return images, masks\n",
    "\n",
    "class Image_Preprocess(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, img_size, imgs, masks):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.imgs = imgs\n",
    "        self.masks = masks\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.imgs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        i = idx * self.batch_size        \n",
    "        batch_img = self.imgs[i : i + self.batch_size]\n",
    "        batch_mask = self.masks[i : i + self.batch_size]\n",
    "\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"float32\")\n",
    "        \n",
    "        for j, path in enumerate(batch_img):\n",
    "            img = path\n",
    "            x[j] = img\n",
    "        \n",
    "        for j, path in enumerate(batch_mask):\n",
    "            img = path\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "            y[j] -= 1\n",
    "        \n",
    "        return x,y\n",
    "\n",
    "class EarlyStoppingByValAcc(Callback):\n",
    "    def __init__(self, monitor='val_sparse_categorical_accuracy', value=0.9998, verbose=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"[INFO] Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current > self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"[INFO] Epoch %05d: early stopping!\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# wandb.init(project=\"your-test-project\", entity=\"YourEntity\")\n",
    "# wandb.config.format = \"tf\"\n",
    "\n",
    "BUT_ID = 200543\n",
    "img_size = (480,640)\n",
    "train_path_img = \"test/test_dataset/images/\"\n",
    "train_path_mask = \"test/test_dataset/masks/\"\n",
    "val_path_img = \"test_dataset/val/images/\"\n",
    "val_path_mask = \"test_dataset/val/masks/\"\n",
    "\n",
    "num_classes = 3\n",
    "batch_size = 4\n",
    "num_epochs = 15\n",
    "backbone = 'mobilenet'\n",
    "optimizer = 'Adam'\n",
    "activation_function = 'softmax'\n",
    "\n",
    "train_img, train_mask = Image_Load().get_data(train_path_img, train_path_mask)\n",
    "train_img, train_mask = Image_Augment().aug_process(train_img, train_mask, num=90, train=True)\n",
    "\n",
    "val_img, val_mask = Image_Load().get_data(val_path_img, val_path_mask)\n",
    "val_img, val_mask = Image_Augment().aug_process(val_img, val_mask, num=10, train=False)\n",
    "\n",
    "random.Random(BUT_ID).shuffle(train_img)\n",
    "random.Random(BUT_ID).shuffle(train_mask)\n",
    "random.Random(BUT_ID).shuffle(val_img)\n",
    "random.Random(BUT_ID).shuffle(val_mask)\n",
    "\n",
    "train_data = Image_Preprocess(batch_size, img_size, train_img, train_mask)\n",
    "val_data = Image_Preprocess(batch_size, img_size, val_img, val_mask)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(),\n",
    "    wandb.keras.WandbCallback(),\n",
    "    keras.callbacks.ModelCheckpoint(\"Unet-Mobilenet\", save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = sm.Unet(backbone, classes=num_classes, activation=activation_function)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics = [keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "train_steps_per_epoch = np.floor(len(train_data) / 4)\n",
    "val_steps_per_epoch = np.floor(len(val_data) / 4)\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "base_model, layers, layer_names = tasm.create_base_model(name=backbone, weights=\"imagenet\", height=img_size[0], width=img_size[1])\n",
    "model = tasm.ASPOCRNet(n_classes=num_classes, base_model=base_model, output_layers=layers, backbone_trainable=False)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics = [keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "train_steps_per_epoch = np.floor(len(train_data) / 4)\n",
    "val_steps_per_epoch = np.floor(len(val_data) / 4)\n",
    "\n",
    "model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
